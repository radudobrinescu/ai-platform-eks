# Example: Deploy Llama 3.1 8B (gated model)
# Prerequisites:
#   kubectl create secret generic hf-token -n inference --from-literal=token=hf_YOUR_TOKEN
apiVersion: kro.run/v1alpha1
kind: InferenceEndpoint
metadata:
  name: llama-8b
  namespace: inference
spec:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  gpuCount: 1
  minReplicas: 1
  maxReplicas: 3
  workerMemory: "24Gi"
  workerCpu: "4"
  maxModelLen: 8192
